AWSTemplateFormatVersion: '2010-09-09'
Description: 'Nextflow AWS Batch Infrastructure'

Parameters:
  VpcCidr: 
    Type: String
    Default: 10.0.0.0/16
    Description: CIDR block for the VPC.
  Subnet1Cidr:
    Type: String
    Default: 10.0.1.0/24
    Description: CIDR block for private subnet 1.
  Subnet2Cidr:
    Type: String
    Default: 10.0.2.0/24
    Description: CIDR block for private subnet 2.
  PublicSubnetCidr:
    Type: String
    Default: 10.0.3.0/24
    Description: CIDR block for public subnet.

Resources:
  # VPC and Networking
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-VPC'

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref PublicSubnetCidr
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PublicSubnet'

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet1Cidr
      AvailabilityZone: !Select [ 0, !GetAZs '' ]
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PrivateSubnet1'

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: !Ref Subnet2Cidr
      AvailabilityZone: !Select [ 1, !GetAZs '' ]
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PrivateSubnet2'

  NatGatewayEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc

  NatGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId: !Ref PublicSubnet

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref PublicRouteTable

  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC

  PrivateRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway

  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnet1
      RouteTableId: !Ref PrivateRouteTable

  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnet2
      RouteTableId: !Ref PrivateRouteTable

  # S3 Bucket for workflow data
  WorkflowBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-nextflow-workdir'
      VersioningConfiguration:
        Status: Enabled
      
  # EFS for shared storage (optional but recommended)
  FileSystem:
    Type: AWS::EFS::FileSystem
    Properties:
      PerformanceMode: generalPurpose
      ThroughputMode: bursting

  # IAM Roles
  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
        - arn:aws:iam::aws:policy/AmazonS3FullAccess # For simplicity, scope down in production
        - arn:aws:iam::aws:policy/AmazonEFSFullAccess # For simplicity, scope down in production

  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles: [!Ref BatchInstanceRole]

  SpotFleetRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: spotfleet.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole

  NextflowJobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaInvokePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !GetAtt GenerateSamplesheetFunction.Arn
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess # For simplicity, scope down in production

  # Security Group
  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Batch instances
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0

  # Batch Compute Environment
  BatchComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      State: ENABLED
      ComputeResources:
        Type: EC2_SPOT  # or EC2 for on-demand
        MinvCpus: 0
        MaxvCpus: 256
        DesiredvCpus: 4
        InstanceTypes:
          - optimal
        Subnets:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        SpotIamFleetRole: !Ref SpotFleetRole
        BidPercentage: 80

  # Batch Job Queue
  BatchJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub '${AWS::StackName}-job-queue'
      Priority: 1
      ComputeEnvironmentOrder:
        - Order: 1
          ComputeEnvironment: !Ref BatchComputeEnvironment

  # Job Definition for the launcher
  NextflowRunnerJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      JobDefinitionName: nextflow-runner
      Type: container
      ContainerProperties:
        Image: nextflow/nextflow:latest-edge # A public Nextflow image
        Vcpus: 1
        Memory: 1024
        Command: [ "/bin/bash", "-c", "nextflow run ${WORKFLOW_URL} -params-file ${PARAMS_FILE} -resume" ]
        JobRoleArn: !Ref NextflowJobRole
        Environment:
          - Name: WORKFLOW_URL
            Value: "ref:workflow"
          - Name: PARAMS_FILE
            Value: "ref:params"

  # DynamoDB Table for experiment contexts
  ExperimentContextsDB:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub '${AWS::StackName}-ExperimentContexts'
      AttributeDefinitions:
        - AttributeName: experiment_id
          AttributeType: S
        - AttributeName: sample_id
          AttributeType: S
      KeySchema:
        - AttributeName: experiment_id
          KeyType: HASH
        - AttributeName: sample_id
          KeyType: RANGE
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5

  # IAM Role for Lambda
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: LambdaDynamoDBAndS3Policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:Query
                Resource: !GetAtt ExperimentContextsDB.Arn
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectTagging
                Resource: !Sub 'arn:aws:s3:::${WorkflowBucket}/*'

  # Lambda Function to generate samplesheet
  GenerateSamplesheetFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-GenerateSamplesheet'
      Handler: lambda_function.handler.generate_samplesheet
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Timeout: 60
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import csv
          from io import StringIO

          dynamodb = boto3.resource('dynamodb')
          s3_client = boto3.client('s3')

          def generate_samplesheet(event, context):
              print(f"Received event: {event}")
              try:
                  body = json.loads(event.get('body', '{}'))
                  experiment_id = body.get('experiment_id')
                  if not experiment_id:
                      raise ValueError("experiment_id not found in request body")
              except (json.JSONDecodeError, ValueError) as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'Error parsing request: {str(e)}')
                  }

              table_name = os.environ.get('DYNAMODB_TABLE')
              bucket_name = os.environ.get('S3_BUCKET')

              if not table_name or not bucket_name:
                  return {
                      'statusCode': 500,
                      'body': json.dumps('Error: DYNAMODB_TABLE or S3_BUCKET environment variables not set.')
                  }

              table = dynamodb.Table(table_name)

              try:
                  response = table.query(
                      KeyConditionExpression='experiment_id = :eid',
                      ExpressionAttributeValues={':eid': experiment_id}
                  )
                  items = response.get('Items', [])
              except Exception as e:
                  return {
                      'statusCode': 500,
                      'body': json.dumps(f'Error querying DynamoDB: {str(e)}')
                  }

              if not items:
                  return {
                      'statusCode': 404,
                      'body': json.dumps(f'No samples found for experiment_id: {experiment_id}')
                  }

              output = StringIO()
              header = ['sample', 'fastq_1', 'fastq_2', 'experimental_group', 'treatment']
              writer = csv.DictWriter(output, fieldnames=header, extrasaction='ignore')
              writer.writeheader()

              for item in items:
                  try:
                      s3_object_key = item.get('s3_object_key')
                      if not s3_object_key:
                          continue

                      tags_response = s3_client.get_object_tagging(
                          Bucket=bucket_name,
                          Key=s3_object_key
                      )
                      s3_tags = {tag['Key']: tag['Value'] for tag in tags_response.get('TagSet', [])}

                      row_data = {
                          'sample': item.get('sample_id'),
                          'fastq_1': f's3://{bucket_name}/{s3_object_key}',
                          'fastq_2': s3_tags.get('fastq_2', ''),
                          'experimental_group': item.get('experimental_group'),
                          'treatment': item.get('treatment')
                      }
                      writer.writerow(row_data)

                  except Exception as e:
                      print(f"Skipping sample {item.get('sample_id')} due to error: {str(e)}")
                      continue

              return {
                  'statusCode': 200,
                  'headers': {'Content-Type': 'text/csv'},
                  'body': output.getvalue()
              }

Outputs:
  VPCId:
    Value: !Ref VPC
  WorkflowBucketName:
    Value: !Ref WorkflowBucket
  BatchJobQueueArn:
    Value: !GetAtt BatchJobQueue.JobQueueArn
  NextflowJobRoleArn:
    Value: !GetAtt NextflowJobRole.Arn
  ExperimentContextsDBName:
    Value: !Ref ExperimentContextsDB
  GenerateSamplesheetLambdaArn:
    Value: !GetAtt GenerateSamplesheetFunction.Arn
