AWSTemplateFormatVersion: '2010-09-09'
Description: 'Nextflow AWS Batch Infrastructure'

Parameters:

  # Batch Parameters
  pEcsAmi:
    Type: "AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>"
    Description: "The ECS-optimized AMI to use for the Batch compute environment"
    Default: "/aws/service/ecs/optimized-ami/amazon-linux-2/recommended/image_id"
  pInstanceTypes:
    Type: "CommaDelimitedList"
    Description: "The instance types to use for the Batch compute environment"
    Default: "optimal"
  pMinVcpus:
    Type: "Number"
    Description: "The minimum number of vCPUs to maintain in the Batch compute environment"
    Default: 0
  pMaxVcpus:
    Type: "Number"
    Description: "The maximum number of vCPUs to scale up to in the Batch compute environment"
    Default: 128
  pDesiredVcpus:
    Type: "Number"
    Description: "The desired number of vCPUs to start with in the Batch compute environment"
    Default: 0

  # IAM Role Parameters (Provide ARNs to use existing roles)
  pBatchInstanceRoleArn:
    Type: String
    Description: "ARN for the Batch Instance Role. If provided, the template will use this existing role."
    Default: ""
  pNextflowJobRoleArn:
    Type: String
    Description: "ARN for the Nextflow Job Role. If provided, the template will use this existing role."
    Default: ""
  pSpotFleetRoleArn:
    Type: String
    Description: "ARN for the Spot Fleet Role. If provided, the template will use this existing role."
    Default: ""
  pLambdaExecutionRoleArn:
    Type: String
    Description: "ARN for the Lambda Execution Role. If provided, the template will use this existing role."
    Default: ""

  # Optional Feature Parameters
  pCreateEFS:
    Type: String
    Description: "Set to 'true' to create an EFS file system for the Batch environment."
    Default: "true"
    AllowedValues: ["true", "false"]
  pCreateFlowLogs:
    Type: String
    Description: "Set to 'true' to create VPC Flow Logs. Requires IAM permissions."
    Default: "false"
    AllowedValues: ["true", "false"]

  pWorkflowBucketName:
    Type: String
    Description: "A unique, lowercase name for the S3 bucket for workflow data. No uppercase letters allowed."
    AllowedPattern: '^[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$'

  # VPC Parameters
  pVpcId:
    Type: AWS::EC2::VPC::Id
    Description: "The ID of the VPC to deploy resources into."
  pPublicSubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: "A list of public subnet IDs for the Batch compute environment."
  pPrivateSubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: "A list of private subnet IDs for the Batch compute environment."
  pPrivateDataSubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: "A list of private data subnet IDs for the EFS file system."


  pDhcpInternalDomain:
    Type: String
    Description: "Custom Internal Domain Name. Specify if Custom Domain parameter is set to true."
    Default: ""
  pDc01Ip:
    Type: String
    Description: "Domain Controller Private IP for DHCP Option Set"
    Default: ""
  pDc02Ip:
    Type: String
    Description: "Domain Controller Private IP for DHCP Option Set"
    Default: ""
  pDc03Ip:
    Type: String
    Description: "Domain Controller Private IP for DHCP Option Set"
    Default: ""
  pFlowlogRetention:
    Type: Number
    Description: "Days VPC Flowlogs are retained in CloudWatch."
    Default: 7
  pS3VpcEndpoint:
    Type: String
    Description: "Enable VPC Endpoint S3"
    Default: true
    AllowedValues: [true, false]
  pEcrEndpoint:
    Type: String
    Description: "Enable VPC Endpoint ECR"
    Default: true
    AllowedValues: [true, false]
  pEnvironmentTag:
    Type: String
    Description: "Environment type for default resource tagging"
    Default: "development"
    AllowedValues: ["development", "staging", "qa", "dr", "sandbox", "production", "central"]

Conditions:
  CondShouldCreateEFS: !Equals [!Ref pCreateEFS, "true"]
  CondShouldCreateFlowLogs: !Equals [!Ref pCreateFlowLogs, "true"]

  # IAM Role Conditions
  CondCreateBatchInstanceRole: !Equals [!Ref pBatchInstanceRoleArn, ""]
  CondCreateNextflowJobRole: !Equals [!Ref pNextflowJobRoleArn, ""]
  CondCreateSpotFleetRole: !Equals [!Ref pSpotFleetRoleArn, ""]
  CondCreateLambdaExecutionRole: !Equals [!Ref pLambdaExecutionRoleArn, ""]

Resources:


  # S3 Bucket for workflow data
  WorkflowBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref pWorkflowBucketName
      VersioningConfiguration:
        Status: Enabled
      
  # EFS for shared storage (optional)
  EfsSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Condition: CondShouldCreateEFS
    Properties:
      GroupDescription: "Security group for EFS"
      VpcId: !Ref pVpcId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 2049
          ToPort: 2049
          SourceSecurityGroupId: !Ref BatchSecurityGroup

  FileSystem:
    Type: AWS::EFS::FileSystem
    Condition: CondShouldCreateEFS
    Properties:
      PerformanceMode: generalPurpose
      ThroughputMode: bursting
      Encrypted: true
      FileSystemTags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-EFS"

  MountTarget1:
    Type: AWS::EFS::MountTarget
    Condition: CondShouldCreateEFS
    Properties:
      FileSystemId: !Ref FileSystem
      SubnetId: !Select [ 0, !Ref pPrivateDataSubnetIds ]
      SecurityGroups: [!Ref EfsSecurityGroup]

  MountTarget2:
    Type: AWS::EFS::MountTarget
    Condition: CondShouldCreateEFS
    Properties:
      FileSystemId: !Ref FileSystem
      SubnetId: !Select [ 1, !Ref pPrivateDataSubnetIds ]
      SecurityGroups: [!Ref EfsSecurityGroup]

  # IAM Roles


  SpotFleetRole:
    Type: AWS::IAM::Role
    Condition: CondCreateSpotFleetRole
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: spotfleet.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole

  NextflowJobRole:
    Type: AWS::IAM::Role
    Condition: CondCreateNextflowJobRole
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaInvokePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !GetAtt GenerateSamplesheetFunction.Arn
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess # For simplicity, scope down in production

  # Security Group
  BatchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Batch instances
      VpcId: !Ref pVpcId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0

  # Batch Compute Environment
  BatchComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      State: ENABLED
      ComputeResources:
        Type: SPOT
        AllocationStrategy: SPOT_CAPACITY_OPTIMIZED
        MinvCpus: !Ref pMinVcpus
        MaxvCpus: !Ref pMaxVcpus
        DesiredvCpus: !Ref pDesiredVcpus
        InstanceTypes: !Ref pInstanceTypes
        ImageId: !Ref pEcsAmi
        Subnets: !Ref pPrivateSubnetIds
        SecurityGroupIds:
          - !Ref BatchSecurityGroup
        InstanceRole: !Ref pBatchInstanceRoleArn
        SpotIamFleetRole: !If [CondCreateSpotFleetRole, !Ref SpotFleetRole, !Ref pSpotFleetRoleArn]

  # Batch Job Queue
  BatchJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub '${AWS::StackName}-job-queue'
      Priority: 1
      ComputeEnvironmentOrder:
        - Order: 1
          ComputeEnvironment: !Ref BatchComputeEnvironment

  # Job Definition for the launcher
  NextflowRunnerJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      JobDefinitionName: nextflow-runner
      Type: container
      ContainerProperties:
        Image: nextflow/nextflow:latest-edge # A public Nextflow image
        Vcpus: 1
        Memory: 1024
        Command: [ "/bin/bash", "-c", "nextflow run ${WORKFLOW_URL} -params-file ${PARAMS_FILE} -resume" ]
        JobRoleArn: !If [CondCreateNextflowJobRole, !Ref NextflowJobRole, !Ref pNextflowJobRoleArn]
        Environment:
          - Name: WORKFLOW_URL
            Value: "ref:workflow"
          - Name: PARAMS_FILE
            Value: "ref:params"

  # DynamoDB Table for experiment contexts
  ExperimentContextsDB:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub '${AWS::StackName}-ExperimentContexts'
      AttributeDefinitions:
        - AttributeName: experiment_id
          AttributeType: S
        - AttributeName: sample_id
          AttributeType: S
      KeySchema:
        - AttributeName: experiment_id
          KeyType: HASH
        - AttributeName: sample_id
          KeyType: RANGE
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5

  # IAM Role for Lambda
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Condition: CondCreateLambdaExecutionRole
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: LambdaDynamoDBAndS3Policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:Query
                Resource: !GetAtt ExperimentContextsDB.Arn
              - Effect: Allow
                Action:
                  - s3:GetObjectTagging
                Resource: !Sub "arn:aws:s3:::${WorkflowBucket}/*"

  # Lambda Function to generate samplesheet
  GenerateSamplesheetFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-GenerateSamplesheet'
      Handler: lambda_function.handler.generate_samplesheet
      Role: !If [CondCreateLambdaExecutionRole, !GetAtt LambdaExecutionRole.Arn, !Ref pLambdaExecutionRoleArn]
      Runtime: python3.9
      Timeout: 60
      Environment:
        Variables:
          DYNAMODB_TABLE: !Ref ExperimentContextsDB
          S3_BUCKET: !Ref WorkflowBucket
      Code:
        ZipFile: |
          import os
          import json
          import boto3
          import csv
          from io import StringIO

          dynamodb = boto3.resource('dynamodb')
          s3_client = boto3.client('s3')

          def generate_samplesheet(event, context):
              print(f"Received event: {event}")
              try:
                  body = json.loads(event.get('body', '{}'))
                  experiment_id = body.get('experiment_id')
                  if not experiment_id:
                      raise ValueError("experiment_id not found in request body")
              except (json.JSONDecodeError, ValueError) as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'Error parsing request: {str(e)}')
                  }

              table_name = os.environ.get('DYNAMODB_TABLE')
              bucket_name = os.environ.get('S3_BUCKET')

              if not table_name or not bucket_name:
                  return {
                      'statusCode': 500,
                      'body': json.dumps('Error: DYNAMODB_TABLE or S3_BUCKET environment variables not set.')
                  }

              table = dynamodb.Table(table_name)

              try:
                  response = table.query(
                      KeyConditionExpression='experiment_id = :eid',
                      ExpressionAttributeValues={':eid': experiment_id}
                  )
                  items = response.get('Items', [])
              except Exception as e:
                  return {
                      'statusCode': 500,
                      'body': json.dumps(f'Error querying DynamoDB: {str(e)}')
                  }

              if not items:
                  return {
                      'statusCode': 404,
                      'body': json.dumps(f'No samples found for experiment_id: {experiment_id}')
                  }

              output = StringIO()
              header = ['sample', 'fastq_1', 'fastq_2', 'experimental_group', 'treatment']
              writer = csv.DictWriter(output, fieldnames=header, extrasaction='ignore')
              writer.writeheader()

              for item in items:
                  try:
                      s3_object_key = item.get('s3_object_key')
                      if not s3_object_key:
                          continue

                      tags_response = s3_client.get_object_tagging(
                          Bucket=bucket_name,
                          Key=s3_object_key
                      )
                      s3_tags = {tag['Key']: tag['Value'] for tag in tags_response.get('TagSet', [])}

                      row_data = {
                          'sample': item.get('sample_id'),
                          'fastq_1': f's3://{bucket_name}/{s3_object_key}',
                          'fastq_2': s3_tags.get('fastq_2', ''),
                          'experimental_group': item.get('experimental_group'),
                          'treatment': item.get('treatment')
                      }
                      writer.writerow(row_data)

                  except Exception as e:
                      print(f"Skipping sample {item.get('sample_id')} due to error: {str(e)}")
                      continue

              return {
                  'statusCode': 200,
                  'headers': {'Content-Type': 'text/csv'},
                  'body': output.getvalue()
              }

Outputs:
  VpcId:
    Description: The VPC ID used for the deployment.
    Value: !Ref pVpcId
  PublicSubnetIds:
    Description: The Public Subnet IDs used for the deployment.
    Value: !Join [",", !Ref pPublicSubnetIds]
  PrivateSubnetIds:
    Description: The Private Subnet IDs used for the deployment.
    Value: !Join [",", !Ref pPrivateSubnetIds]
  PrivateDataSubnetIds:
    Description: The Private Data Subnet IDs used for the deployment.
    Value: !Join [",", !Ref pPrivateDataSubnetIds]
  BatchJobQueueArn:
    Description: ARN of the Batch Job Queue
    Value: !Ref BatchJobQueue
  NextflowRunnerJobDefinitionArn:
    Description: ARN of the Nextflow Runner Job Definition
    Value: !Ref NextflowRunnerJobDefinition
  DynamoDBTableName:
    Description: Name of the DynamoDB table for experiment contexts
    Value: !Ref ExperimentContextsDB
  S3BucketName:
    Description: Name of the S3 bucket for workflow data
    Value: !Ref WorkflowBucket
  LambdaFunctionName:
    Description: Name of the Lambda function for generating samplesheets
    Value: !Ref GenerateSamplesheetFunction
  EFSFileSystemId:
    Description: ID of the EFS File System
    Condition: CondShouldCreateEFS
    Value: !Ref FileSystem
